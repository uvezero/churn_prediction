{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    "    mean_absolute_error,\n",
    "    log_loss,\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.xgboost\n",
    "\n",
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK, STATUS_FAIL\n",
    "from hyperopt.pyll import scope\n",
    "from catboost import CatBoostClassifier\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(\"churn-predictor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/.../dataset.csv')\n",
    "df = df.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.select_dtypes('object').columns:\n",
    "    df[col] = df[col].astype('category')\n",
    "    \n",
    "X = df.drop('Churn', axis=1)\n",
    "y = df['Churn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable autologging for sklearn\n",
    "mlflow.sklearn.autolog()\n",
    "\n",
    "\n",
    "def objective(params):\n",
    "    try:\n",
    "        # Extract StratifiedShuffleSplit parameters from the search space\n",
    "        n_splits = int(params['n_splits'])\n",
    "        test_size = params['test_size']\n",
    "        \n",
    "        # Remove StratifiedShuffleSplit parameters from the XGBoost parameters\n",
    "        xgb_params = {k: params[k] for k in params if k not in ['n_splits', 'test_size', 'num_boost_round']}\n",
    "        \n",
    "        xgb_params['max_depth'] = int(xgb_params['max_depth'])\n",
    "        xgb_params['objective'] = 'binary:logistic'  # Set objective for binary classification\n",
    "        xgb_params['eval_metric'] = 'logloss'  # Set evaluation metric\n",
    "        \n",
    "        sss = StratifiedShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=42)\n",
    "        train_index, test_index = next(sss.split(X, y))  # Use the first split\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        with mlflow.start_run():\n",
    "            mlflow.set_tag(\"model\", \"xgboost\")\n",
    "            mlflow.log_params(xgb_params)\n",
    "            \n",
    "            dtrain = xgb.DMatrix(X_train, label=y_train, enable_categorical=True)  # Enable categorical handling\n",
    "            dtest = xgb.DMatrix(X_test, label=y_test, enable_categorical=True)\n",
    "            \n",
    "            booster = xgb.train(\n",
    "                params=xgb_params,\n",
    "                dtrain=dtrain,\n",
    "                num_boost_round=int(params['num_boost_round']),\n",
    "                evals=[(dtest, 'validation')],\n",
    "                early_stopping_rounds=50,\n",
    "                verbose_eval=False  # Turn off verbose output\n",
    "            )\n",
    "            \n",
    "            # Save the model manually in JSON format\n",
    "            model_path = \"xgboost_model.json\"\n",
    "            booster.save_model(model_path)  # Save model as JSON\n",
    "\n",
    "            # Log the model manually with MLflow\n",
    "            mlflow.log_artifact(model_path, artifact_path=\"xgboost_model\")\n",
    "\n",
    "            y_pred_proba = booster.predict(dtest)\n",
    "            \n",
    "            # Binary class predictions\n",
    "            y_pred = [1 if prob > 0.5 else 0 for prob in y_pred_proba]\n",
    "            \n",
    "            logloss = log_loss(y_test, y_pred_proba)\n",
    "            auc = roc_auc_score(y_test, y_pred_proba)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            precision = precision_score(y_test, y_pred)\n",
    "            recall = recall_score(y_test, y_pred)\n",
    "            \n",
    "            mlflow.log_metric(\"log_loss\", logloss)\n",
    "            mlflow.log_metric(\"auc\", auc)\n",
    "            mlflow.log_metric(\"accuracy\", accuracy)\n",
    "            mlflow.log_metric(\"precision\", precision)\n",
    "            mlflow.log_metric(\"recall\", recall)\n",
    "            \n",
    "            # Remove the saved model file after logging\n",
    "            os.remove(model_path)\n",
    "\n",
    "        return {'loss': logloss, 'status': STATUS_OK}\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return {'loss': float('inf'), 'status': STATUS_FAIL}\n",
    "\n",
    "\n",
    "def objective_catboost(params):\n",
    "    # Extract StratifiedShuffleSplit parameters from the search space\n",
    "    n_splits = params['n_splits']\n",
    "    test_size = params['test_size']\n",
    "    \n",
    "    # StratifiedShuffleSplit with the sampled parameters\n",
    "    sss = StratifiedShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=42)\n",
    "    train_index, test_index = next(sss.split(X, y))  # Use the first split\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    X_train.to_csv('')\n",
    "\n",
    " \n",
    "    with mlflow.start_run():\n",
    "        mlflow.set_tag(\"model\", \"catboost\")\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        model = CatBoostClassifier(\n",
    "            iterations=int(params['iterations']),\n",
    "            depth=int(params['depth']),\n",
    "            learning_rate=params['learning_rate'],\n",
    "            l2_leaf_reg=params['l2_leaf_reg'],\n",
    "            random_seed=42,\n",
    "            verbose=0\n",
    "        )\n",
    "        model.fit(X_train, y_train, cat_features=categorical_features)\n",
    "        \n",
    "        # Log model\n",
    "        mlflow.catboost.log_model(model, \"catboost_model\")\n",
    "        \n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        logloss = log_loss(y_test, y_pred_proba)\n",
    "        auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        \n",
    "        # Log metrics with MLflow\n",
    "        mlflow.log_metric(\"log_loss\", logloss)\n",
    "        mlflow.log_metric(\"auc\", auc)\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        mlflow.log_metric(\"precision\", precision)\n",
    "        mlflow.log_metric(\"recall\", recall)\n",
    "\n",
    "    return {'loss': logloss, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "\n",
    "# Function to preprocess data by encoding categorical features\n",
    "def preprocess_data(X):\n",
    "    # Identify categorical columns\n",
    "    X_encoded = X.copy()\n",
    "    categorical_cols = X_encoded.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    for col in categorical_cols:\n",
    "        if X_encoded[col].nunique() == 2:  \n",
    "            X_encoded[col] = label_encoder.fit_transform(X_encoded[col])\n",
    "        else:\n",
    "            # Multi-class columns\n",
    "            X_encoded = pd.get_dummies(X_encoded, columns=[col], drop_first=True)\n",
    "\n",
    "    return X_encoded\n",
    "\n",
    "# Xgboost searchspace for MLflow\n",
    "search_space = {\n",
    "    'n_splits': hp.choice('n_splits', [5, 10]),  # Number of splits for StratifiedShuffleSplit\n",
    "    'test_size': hp.choice('test_size', [0.2, 0.3]),  # Test size for StratifiedShuffleSplit\n",
    "    'max_depth': hp.quniform('max_depth', 3, 10, 1),  # Maximum depth of each tree\n",
    "    'learning_rate': hp.loguniform('learning_rate', -5, 0),  # Learning rate (eta)\n",
    "    'num_boost_round': hp.quniform('num_boost_round', 100, 1000, 50),  # Number of boosting rounds\n",
    "    'min_child_weight': hp.uniform('min_child_weight', 1, 10),  # Minimum sum of instance weight (Hessian)\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1.0),  # Subsample ratio of the training instances\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.3, 1.0),  # Subsample ratio of columns\n",
    "    'gamma': hp.uniform('gamma', 0, 5),  # Minimum loss reduction\n",
    "    'reg_lambda': hp.loguniform('reg_lambda', -3, 3),  # L2 regularization term\n",
    "    'reg_alpha': hp.loguniform('reg_alpha', -3, 3),  # L1 regularization term\n",
    "}\n",
    "\n",
    "# Catboost searchspace for MLflow\n",
    "search_space_catboost = {'n_splits': hp.choice('n_splits', [5, 10]), \n",
    "    'test_size': hp.choice('test_size', [0.2, 0.3]), \n",
    "    'iterations': scope.int(hp.quniform('iterations', 100, 1000, 50)), \n",
    "    'depth': scope.int(hp.quniform('depth', 4, 12, 1)), \n",
    "    'learning_rate': hp.loguniform('learning_rate', -5, 0),  \n",
    "    'l2_leaf_reg': hp.loguniform('l2_leaf_reg', 0, 3), \n",
    "    'bagging_temperature': hp.uniform('bagging_temperature', 0, 1), \n",
    "    'border_count': scope.int(hp.quniform('border_count', 32, 254, 1)),\n",
    "    'rsm': hp.uniform('rsm', 0.5, 1),  \n",
    "    'grow_policy': hp.choice('grow_policy', ['SymmetricTree', 'Depthwise', 'Lossguide']),  \n",
    "    'sampling_frequency': hp.choice('sampling_frequency', ['PerTree', 'PerTreeLevel']),  \n",
    "    'od_wait': scope.int(hp.quniform('od_wait', 20, 100, 10)), \n",
    "    'loss_function': hp.choice('loss_function', ['Logloss', 'CrossEntropy', 'RMSE', 'MAE']), \n",
    "    'scale_pos_weight':scope.int(hp.quniform('scale_pos_weight', 1, 5, 1)) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost\n",
    "best_result_catboost = fmin(\n",
    "    fn=objective_catboost,\n",
    "    space=search_space_catboost,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=50,\n",
    "    trials=Trials()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost\n",
    "trials = Trials()\n",
    "best_result = fmin(\n",
    "    fn=objective,\n",
    "    space=search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=50,\n",
    "    trials=trials\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
